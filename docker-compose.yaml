version: '3.8'

services:

  ollama:
    image: ollama/ollama
    container_name: ollama

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - local-ai-net
    restart: unless-stopped

  open-webui:

    build:
      context: ./open-webui
      args:
        - USE_CUDA=true
        - USE_CUDA_VER=cu121
       
    container_name: open-webui-custom 
    depends_on:
      - ollama
    ports:
      - "3000:8080"
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
    volumes:
      # Сохраняем данные RAG в папку ./webui-data
      - ./webui-data:/app/backend/data
    networks:
      - local-ai-net
    restart: unless-stopped

networks:
  local-ai-net:
    driver: bridge